## 模型
### 聚类
聚类首先是无监督学习，也就是样本标记信息是未知的，聚类试图将数据集中的样本划分为若干个通常是不相交的子集,每个子集称为簇，通过分类，每个簇可能会有同样的特征
 

#### 聚类使用场景
1.商业应用中需对新用户的类型进行判别，对于定义用户类型，可能不太容易，则可以通过对用户数据进行聚类，根据聚类结果将每个簇定义为一个类，


####说明
迭代：选质心，更新质心，，若目标归位2类，随机选择两个质心，计算每个样本与质心的距离，选择最近的一个质心作为类别，全部算完，之后，重新在算质心，若质心变化之后，则进行下一轮迭代，重新计算，在更新，直到质心不更新

```python
## 寻找最合适的分类
indicator_filter=['consum_in90day','consum_in90day_avg','consum_day_max_in90day_avg']
data_extract_ss3=(data_src[indicator_filter]-data_src[indicator_filter].min())/(data_src[indicator_filter].max()-data_src[indicator_filter].min())


result_dic=[]
##存放轮廓系数
result_scores=[]

for k in range(2,15):
    result_dic=result_dic
    kmodel=KMeans(n_clusters=k)
    kmodel.fit(data_extract_ss3)
    kmodel.cluster_centers_
    labels=kmodel.labels_
    num_k=kmodel.inertia_
    result_dic.append(num_k)
    result_scores.append(silhouette_score(data_extract_ss3,labels,metric='euclidean'))
    
data_src['类别']=labels
data_extract_ss3['类别']=labels
  
    
plt.figure(figsize=(20,5))
plt.plot(range(2,15),result_dic,marker='o',label='簇内误方差(SSE)')
# plt.plot(range(2,15),result_scores,marker='o',color='green',label='轮廓系数')
plt.xlabel("簇数量")
plt.ylabel("簇内误方差(SSE)")
plt.title('手肘图')
plt.show()


plt.figure(figsize=(20,5))
plt.plot(range(2,15),result_scores,marker='o',color='green',label='轮廓系数')
plt.xlabel("簇数量")
plt.ylabel("轮廓系数")
plt.title('轮廓系数')
plt.show()    
        
```

### 随机森林
随机森林是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类，然后看看哪一类被选择最多，就预测这个样本为那一类。


在建立每一棵决策树的过程中，有两点需要注意采样与完全分裂。首先是两个随机采样的过程，random forest对输入的数据要进行行、列的采样。对于行采样，采用有放回的方式，也就是在采样得到的样本集合中，可能有重复的样本。假设输入样本为N个，那么采样的样本也为N个。这样使得在训练的时候，每一棵树的输入样本都不是全部的样本，使得相对不容易出现over-fitting。然后进行列采样，从M个feature中，选择m个（m << M）。之后就是对采样之后的数据使用完全分裂的方式建立出决策树，这样决策树的某一个叶子节点要么是无法继续分裂的，要么里面的所有样本的都是指向的同一个分类。一般很多的决策树算法都一个重要的步骤——剪枝，但是这里不这样干，由于之前的两个随机采样的过程保证了随机性，所以就算不剪枝，也不会出现over-fitting。


具体实现过程如下：

（1）原始训练集为N，应用bootstrap法有放回地随机抽取k个新的自助样本集，并由此构建k棵分类树，每次未被抽到的样本组成了k个袋外数据；

（2）设有mall个变量，则在每一棵树的每个节点处随机抽取mtry个变量(mtry n mall)，然后在mtry中选择一个最具有分类能力的变量，变量分类的阈值通过检查每一个分类点确定；

（3）每棵树最大限度地生长, 不做任何修剪；

（4）将生成的多棵分类树组成随机森林，用随机森林分类器对新的数据进行判别与分类，分类结果按树分类器的投票多少而定。


###  决策树

**Q1，到底使用优先哪个特征**
计算每个特征值划分数据获得的信息增益的变化来划分，获得信息增益最高的特征就是最好的选择。
**Q2，什么是信息增益**
划分数据集之前之后信息发生的变化称为信息增益
**Q3，怎么计算信息增益(information gain)**
分类前的信息熵－分类后的信息熵，为信息增益
**Q4，什么是熵(entropy)**
熵定义为信息的期望值，计算公式如下
![Alt text](./1564821092821.png)
**Q5，决策树的有几个算法**
1. ID3算法：基于信息增益算法，可能会出现的问题是，若某个特征特别稀疏，属性特别多，且基于所有属性划分后，数据会很纯，信息增益就很大，大概率会选择这个特征进行划分，比如把编号误操作选到了特征里，则每一个id下，信息熵均为0，信息增益就很大
2. C4.5：基于信息增益率算法（解决信息增益出现的问题,考虑了自身的熵值），信息增益率＝信息增益／分类前的熵值
3. CART:基于GINI系数当衡量标准
![Alt text](./1564831716239.png)

**Q6，若特征为连续值，是怎么处理**
将连续值进行二分法（离散化），原理是：将连续值进行排序，然后在遍历每个属性值进行二分法，也就是是大于当前属性值还是小于当前属性值（属性值由两个排序后的属性值的平均值决定；
**Q7，剪枝策略**
为何要剪枝？为了解决过拟合（过拟合的表现：训练集上效果很好，测试集上效果一般）
预剪枝：限制深度(树的层)，叶子节点个数，叶子节点样本数，信息增益量等
后剪枝：不了解，先不加

`一行简单的python代码，计算输入数据的信息熵`
简单来说：数据越杂乱，数据的不确定性越高，数据的信息熵越高
```python
def calcShannonEnt(ds):
    numEntries=len(ds) ## 获取输入数据的长度
    labelCounts={} ## 定义一个字典，用来存储标签出现的数量，key是标签，value是次数
    
    ###以下先便利所有的数据，获取标签，默认情况下，数据的最后一列均存储的是该条纪录的表起啊
    for i in ds:
        cLable=i[-1] ## 按照索引取标签
        
        ###  判断获取的标签是否之前出现过，若未出现则标签字典里新增该key，并给默认value为0
        if cLable not in labelCounts.keys(): 
            labelCounts[cLable]=0
        labelCounts[cLable]+=1
    
    shannnonEnt=0.0 ### 定义一个信息熵的变量，初始值为0
    for key in labelCounts:
        prob=float(labelCounts[key])/numEntries ## 计算每个标签占总纪录的比例
        shannnonEnt-=prob * log(prob,2) ## 计算信息熵
        
    ## 返回计算的信息熵
    return shannnonEnt  

myDat1=[[1,1,'yes'],[1,1,'yes'],[1,0,'no'],[0,1,'no'],[0,1,'no']]
myDat2=[[1,1,'maybe'],[1,1,'yes'],[1,0,'no'],[0,1,'no'],[0,1,'no']]
myDat3=[[1,1,'no'],[1,1,'no'],[1,0,'yes'],[0,1,'yes'],[0,1,'yes']]
print(calcShannonEnt(myDat1),calcShannonEnt(myDat2),calcShannonEnt(myDat3))
```
`一行简单的python代码，计算输信息熵与分类占比的关系，以下仅为2分类标签的变化`
```sql
import matplotlib.pyplot as plt
test_data=[] 
for i in range(101):
    test_data.append(i/100)
h_data=[] 
for j in test_data:
    if j==0 or j==1:
        h_data.append(0)
    else:
        i=1-j
        h_data.append((-j*log(j,2)-i*log(i,2)))

#         if j==0.5:
#             print(-j*log(j,2))

plt.plot(test_data,h_data)
plt.xlabel('P值')
plt.ylabel('熵值')
plt.title('P值与信息熵的关系')
plt.show()

```
![Alt text](./1564825721242.png)
**Q8，一个例子**
数据源：14天的打球数据，有4个特征，均为分类数据，y值是是否打球
![Alt text](./1564831371349.png)
划分节点：遍历所有的特征，根据特征的属性值划分，分别结算划分后的信息熵
![Alt text](./1564831462464.png)
信息熵：基于天气划分后的信息熵如下，每个属性值划分后均有不同的信息熵
![Alt text](./1564831548000.png)
每个特征划分的信息熵：各属性值划分后的信息熵加权和
![Alt text](./1564831638454.png)
依次类推，计算所有特征划分后的信息熵，选择信息增益最大的，思路是给予ID3算法，



**Q9，如何sklearn来构造决策树(分类)**

`参数说明`
```bash
from sklearn.tree import DecisionTreeClassifier
'''
分类决策树
'''
DecisionTreeClassifier(criterion="gini",
                 splitter="best",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features=None,
                 random_state=None,
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 class_weight=None,
                 presort=False)
'''
参数含义：
1.criterion:string, optional (default="gini")
            (1).criterion='gini',分裂节点时评价准则是Gini指数。
            (2).criterion='entropy',分裂节点时的评价指标是信息增益。     
2.max_depth:int or None, optional (default=None)。指定树的最大深度。
            如果为None，表示树的深度不限。直到所有的叶子节点都是纯净的，即叶子节点
            中所有的样本点都属于同一个类别。或者每个叶子节点包含的样本数小于min_samples_split，特征多的时候最好设置。
3.splitter:string, optional (default="best")。指定分裂节点时的策略。
           (1).splitter='best',表示选择最优的分裂策略（遍历所有）。
           (2).splitter='random',表示选择最好的随机切分策略。
4.min_samples_split:int, float, optional (default=2)。表示分裂一个内部节点需要的做少样本数。
           (1).如果为整数，则min_samples_split就是最少样本数。
           (2).如果为浮点数(0到1之间)，则每次分裂最少样本数为ceil(min_samples_split * n_samples)
5.min_samples_leaf: int, float, optional (default=1)。指定每个叶子节点需要的最少样本数。
           (1).如果为整数，则min_samples_split就是最少样本数。
           (2).如果为浮点数(0到1之间)，则每个叶子节点最少样本数为ceil(min_samples_leaf * n_samples)
6.min_weight_fraction_leaf:float, optional (default=0.)
           指定叶子节点中样本的最小权重。
7.max_features:int, float, string or None, optional (default=None).
           搜寻最佳划分的时候考虑的特征数量。
           (1).如果为整数，每次分裂只考虑max_features个特征。
           (2).如果为浮点数(0到1之间)，每次切分只考虑int(max_features * n_features)个特征。
           (3).如果为'auto'或者'sqrt',则每次切分只考虑sqrt(n_features)个特征
           (4).如果为'log2',则每次切分只考虑log2(n_features)个特征。
           (5).如果为None,则每次切分考虑n_features个特征。
           (6).如果已经考虑了max_features个特征，但还是没有找到一个有效的切分，那么还会继续寻找
           下一个特征，直到找到一个有效的切分为止。
8.random_state:int, RandomState instance or None, optional (default=None)
           (1).如果为整数，则它指定了随机数生成器的种子。
           (2).如果为RandomState实例，则指定了随机数生成器。
           (3).如果为None，则使用默认的随机数生成器。
9.max_leaf_nodes: int or None, optional (default=None)。指定了叶子节点的最大数量。
           (1).如果为None,叶子节点数量不限。
           (2).如果为整数，则max_depth被忽略。
10.min_impurity_decrease:float, optional (default=0.)
         如果节点的分裂导致不纯度的减少(分裂后样本比分裂前更加纯净)大于或等于min_impurity_decrease，则分裂该节点。
         加权不纯度的减少量计算公式为：
         min_impurity_decrease=N_t / N * (impurity - N_t_R / N_t * right_impurity
                            - N_t_L / N_t * left_impurity)
         其中N是样本的总数，N_t是当前节点的样本数，N_t_L是分裂后左子节点的样本数，
         N_t_R是分裂后右子节点的样本数。impurity指当前节点的基尼指数，right_impurity指
         分裂后右子节点的基尼指数。left_impurity指分裂后左子节点的基尼指数。
11.min_impurity_split:float
         树生长过程中早停止的阈值。如果当前节点的不纯度高于阈值，节点将分裂，否则它是叶子节点。
         这个参数已经被弃用。用min_impurity_decrease代替了min_impurity_split。
 
12.class_weight:dict, list of dicts, "balanced" or None, default=None
         类别权重的形式为{class_label: weight}
         (1).如果没有给出每个类别的权重，则每个类别的权重都为1。
         (2).如果class_weight='balanced'，则分类的权重与样本中每个类别出现的频率成反比。
         计算公式为：n_samples / (n_classes * np.bincount(y))
         (3).如果sample_weight提供了样本权重(由fit方法提供)，则这些权重都会乘以sample_weight。
13.presort:bool, optional (default=False)
        指定是否需要提前排序数据从而加速训练中寻找最优切分的过程。设置为True时，对于大数据集
        会减慢总体的训练过程；但是对于一个小数据集或者设定了最大深度的情况下，会加速训练过程。
 
属性:
1.classes_:array of shape = [n_classes] or a list of such arrays
        类别的标签值。
2.feature_importances_ : array of shape = [n_features]
        特征重要性。越高，特征越重要。
        特征的重要性为该特征导致的评价准则的（标准化的）总减少量。它也被称为基尼的重要性
3.max_features_ : int
        max_features的推断值。
4.n_classes_ : int or list
        类别的数量
5.n_features_ : int
        执行fit后，特征的数量
6.n_outputs_ : int
        执行fit后，输出的数量
 
7.tree_ : Tree object
        树对象，即底层的决策树。
 
方法:
1.fit(X,y):训练模型。
2.predict(X):预测
3.predict_log_poba(X):预测X为各个类别的概率对数值。
4.predict_proba(X):预测X为各个类别的概率值。
 
 
```

#### 画图
```python%matplotlib inline 
## 使用%matplotlib命令可以将matplotlib的图表直接嵌入到Notebook之中，
##或者使用指定的界面库显示图表，它有一个参数指定matplotlib图表的显示方式。inline表示将图表嵌入到Notebook中。
import matplotlib.pyplot as plt 
import pandas as pd

from sklearn.datasets.california_housing import fetch_california_housing 
housing=fetch_california_housing()



from sklearn import tree 
## 实例化参数
dtr=tree.DecisionTreeRegressor(max_depth=2)
dtr.fit(housing.data[:,[6,7]],housing.target) ##前段页面会返回的是一系列参数


## 生成.dot文件
dot_data= \
    tree.export_graphviz(
        dtr,## 实例化，需要按需要修改
        out_file=None,
        feature_names=housing.feature_names[6:8],## 特征值，选择的特征值，需要按需修改
        filled=True,
        impurity=False,
        rounded=True
    )


## 画图
import pydotplus
graph=pydotplus.graph_from_dot_data(dot_data)
graph.get_nodes()[7].set_fillcolor('#FFF2DD')
from IPython.display import Image 
Image(graph.create_png())


graph.write_png('一个图.png')
```


```python
### 数据分为训练集和测试集
from sklearn.model_selection import train_test_split 
data_train,data_test,target_train,target_test= \
    train_test_split(housing.data,housing.target,test_size=0.1,random_state=42)
dtr=tree.DecisionTreeRegressor(random_state=42) 
dtr.fit(data_train,target_train)
dtr.score(data_test,target_test) 


from sklearn.ensemble import RandomForestRegressor
rfr=RandomForestRegressor(random_state=42)
rfr.fit(data_train,target_train)
rfr.score(data_test,target_test)
```

```python
from sklearn.model_selection import GridSearchCV
tree_param_grid={'min_samples_split':list((3,4,9))}
grid=GridSearchCV(RandomForestRegressor(),param_grid=tree_param_grid,cv=5)
grid.fit(data_train,target_train)
grid.score,grid.best_params_,grid.best_score_
```


###  逻辑回归
目的：分类算法 
